Video_Presentation,Abstracts
http://videolectures.net/eswc2019_haase_graph_kaleidoscope/,"In this talk I will provide a perspective on the history, current state and trends of Knowledge Graphs.  This perspective will range from personal experience in academic research in the Semantic Web community  to applications and uptake of knowledge graphs in industry. The talk will be presented as a knowledge graph itself ‚Äì  with real-life data and examples to be interactively explored and to see what is possible with knowledge graph technology today. Peter has a long history in the Semantic Web community, his first contacts with semantic technologies dating back 20 years.  In 2006, Peter obtained his PhD from the University of Karlsruhe (now KIT) under the supervision of Prof. Rudi Studer.  He then worked as Head of Research and Development at fluid Operations, a startup in the area of semantic data management and enterprise cloud management. In 2014, Peter founded metaphacts, focusing on enterprise knowledge graphs. Share on"
http://videolectures.net/eswc2019_maynard_value_nlp/,"Natural language processing technology is now ubiquitous, even if there are still many challenges to be faced in its development. From sentiment analysis to machine translation to chatbots; from medical systems to online shopping to fake news; even if not visibly apparent, NLP tools are now lurking hidden in the depths of an enormous number and range of real-world systems and applications. Techniques have advanced in many directions in the last 20 years thanks primarily to developments in machine learning and deep learning technologies, and to the concomitant creation of and attention to language resources. In this talk, I shall examine the role of semantics in NLP applications. It is perhaps surprising that despite recent advances in Semantic Web technology and NLP, researchers and practitioners in domains ranging from journalism to rocket science have yet to grasp their potential, and are still manually grappling feral spreadsheets and databases. I shall discuss how NLP infused with even small amounts of semantics can be a drastic game-changer in fields as diverse as crisis communication, politics, journalism, medicine, literature, and history, using examples from some of our recent projects and applications."
http://videolectures.net/eswc2019_quercia_healthy_cities/,"We will see how to aggregate both readings from consumer wearable devices and records of food purchases to track people‚Äôs well-being at scale. From 11,600 Nokia Health wearables, we collected readings of steps, sleep, and heart rate in the entire cities of London and San Francisco over the course of 1 year. Christmas and New Year‚Äôs eve were associated only with short-lived and minor disruptions, while both Brexit and Trump‚Äôs election greatly impacted people‚Äôs sleep and even heart rates. Then, for another entire year in London, we studied the association between food purchases in grocery stores, as measured by the digital traces of customer loyalty cards, and consumption of medicines. Our results show that analytics of digital records of grocery purchases can be used as a cheap and scalable tool for health surveillance: the distribution of the food nutrients is far more predictive of food-related illnesses (e.g., diabetes) than socio-economic conditions. Professor of Urban Informatics at King‚Äôs College London, Department Head at Nokia Bell Labs, and co-founder of GoodCityLife.org, Daniele Quercia is a computer scientist, named one of Fortune magazine‚Äôs 2014 Data All-Stars. His research in the area of urban computing has been published in leading venues including ICSE, Ubicomp, ICDM, CSCW, RecSys, WSDM, and WWW, received honorable mentions from AAAI ICWSM, and has been featured on La Repubblica, The Independent, New Scientist, Le Monde, and BBC. He spoke at TEDx Barcelona and Falling Walls Berlin, and wrote for BBC. He was Research Scientist at Yahoo Labs, a Horizon senior researcher at The Computer Laboratory of the University of Cambridge, and Postdoctoral Associate at the Massachusetts Institute of Technology. He received his PhD from UC London. His thesis was sponsored by Microsoft Research Cambridge and was nominated for BCS Best British PhD dissertation in Computer Science. During his PhD, he was MBA Technology Fellow at London Business School."
http://videolectures.net/eswc2019_ayala_aynec_graphs/,"The popularity of knowledge graphs has led to the development of techniques to refine them and increase their quality. One of the main refinement tasks is completion (also known as link prediction for knowledge graphs), which seeks to add missing triples to the graph, usually by classifying potential ones as true or false. While there is a wide variety of graph completion techniques, there is no standard evaluation setup, so each proposal is evaluated using different datasets and metrics. In this paper we present AYNEC, a suite for the evaluation of knowledge graph completion techniques that covers the entire evaluation workflow. It includes a customisable tool for the generation of datasets with multiple variation points related to the preprocessing of graphs, the splitting into training and testing examples, and the generation of negative examples. AYNEC also provides a visual summary of the graph and the optional exportation of the datasets in an open format for their visualisation. We use AYNEC to generate a library of datasets ready to use for evaluation purposes based on several popular knowledge graphs. Finally, it includes a tool that computes relevant metrics and uses significance tests to compare each pair of techniques. These open source tools, along with the datasets, are freely available to the research community and will be maintained."
http://videolectures.net/eswc2019_fathalla_eventskg_star/,"Metadata of scientific events has become increasingly available on the Web, albeit often as raw data in various formats, disregarding its semantics and interlinking relations. This leads to restricting the usability of this data for, e.g., subsequent analyses and reasoning. Therefore, there is a pressing need to represent this data in a semantic representation, i.e., Linked Data. We present the new release of the EVENTSKG dataset, comprising comprehensive semantic descriptions of scientific events of eight computer science communities. Currently, EVENTSKG is a 5-star dataset containing metadata of 73 top-ranked event series (almost 2,000 events) established over the last five decades. The new release is a Linked Open Dataset adhering to an updated version of the Scientific Events Ontology, a reference ontology for event metadata representation, leading to richer and cleaner data. To facilitate the maintenance of EVENTSKG and to ensure its sustainability, EVENTSKG is coupled with a Java API that enables users to add/update events metadata without going into the details of the representation of the dataset. We shed light on events characteristics by analyzing EVENTSKG data, which provides a flexible means for customization in order to better understand the characteristics of renowned CS events."
http://videolectures.net/eswc2019_zloch_software_framework/,"As the availability and the inter-connectivity of RDF datasets grow, so does the necessity to understand the structure of the data. Understanding the topology of RDF graphs can guide and inform the development of, e.g. synthetic dataset generators, sampling methods, index structures, or query optimizers. In this work, we propose two resources: (i) a software framework (Resource URL of the framework: https://doi.org/10.5281/zenodo.2109469) able to acquire, prepare, and perform a graph-based analysis on the topology of large RDF graphs, and (ii) results on a graph-based analysis of 280 datasets (Resource URL of the datasets: https://doi.org/10.5281/zenodo.1214433) from the LOD Cloud with values for 28 graph measures computed with the framework. We present a preliminary analysis based on the proposed resources and point out implications for synthetic dataset generators. Finally, we identify a set of measures, that can be used to characterize graphs in the Semantic Web."
http://videolectures.net/eswc2019_rizzo_boosting_concepts/,"We present a method for boosting relational classifiers of individual resources in the context of the Web of Data. We show how weak classifiers induced by simple concept learners can be enhanced producing strong classification models from training datasets. Even more so the comprehensibility of the model is to some extent preserved as it can be regarded as a sort of concept in disjunctive form. We demonstrate the application of this approach to a weak learner that is easily derived from learners that search a space of hypotheses, requiring an adaptation of the underlying heuristics to take into account weighted training examples. An experimental evaluation on a variety of artificial learning problems and datasets shows that the proposed approach enhances the performance of the basic learners and is competitive, outperforming current concept learning systems."
http://videolectures.net/eswc2019_gupta_semantic_aspects/,"Large document collections can be hard to explore if the user presents her information need in a limited set of keywords. Ambiguous intents arising out of these short queries often result in long-winded query sessions and many query reformulations. To alleviate this problem, in this work, we propose the novel concept of semantic aspects (e.g.,   ‚ü®{ùóÜùóÇùñºùóÅùñ∫ùñæùóÖ\ùóçùñæùóëùóç {-}ùóâùóÅùñæùóÖùóâùóå},{ùñ∫ùóçùóÅùñæùóáùóå, ùñªùñæùóÇùóÉùóÇùóáùóÄ, ùóÖùóàùóáùñΩùóàùóá},[2004,2016]‚ü©  for the ambiguous query  Open image in new window) and present the xFactor algorithm that generates them from annotations in documents. Semantic aspects uplift document contents into a meaningful structured representation, thereby allowing the user to sift through many documents without the need to read their contents. The semantic aspects are created by the analysis of semantic annotations in the form of temporal, geographic, and named entity annotations. We evaluate our approach on a novel testbed of over 5,000 aspects on Web-scale document collections amounting to more than 450 million documents. Our results show the xFactor algorithm finds relevant aspects for highly ambiguous queries."
http://videolectures.net/eswc2019_buron_reformulation_ontologies/,"Query answering in RDF knowledge bases has traditionally been performed either through graph saturation, i.e., adding all implicit triples to the graph, or through query reformulation, i.e., modifying the query to look for the explicit triples entailing precisely what the original query asks for. The most expressive fragment of RDF for which Reformulation-based quey answering exists is the so-called database fragment [12], in which implicit triples are restricted to those entailed using an RDFS ontology. Within this fragment, query answering was so far limited to the interrogation of data triples (non-RDFS ones); however, a powerful feature specific to RDF is the ability to query data and schema triples together. In this paper, we address the general query answering problem by reducing it, through a pre-query reformulation step, to that solved by the query reformulation technique of [12]. We also report on experiments demonstrating the low cost of our reformulation algorithm."
http://videolectures.net/eswc2019_thornton_shape_expressions/,"We discuss Shape Expressions (ShEx), a concise, formal, modeling and validation language for RDF structures. For instance, a Shape Expression could prescribe that subjects in a given RDF graph that fall into the shape ‚ÄúPaper‚Äù are expected to have a section called ‚ÄúAbstract‚Äù, and any ShEx implementation can confirm whether that is indeed the case for all such subjects within a given graph or subgraph. There are currently five actively maintained ShEx implementations. We discuss how we use the JavaScript, Scala and Python implementations in RDF data validation workflows in distinct, applied contexts. We present examples of how ShEx can be used to model and validate data from two different sources, the domain-specific Fast Healthcare Interoperability Resources (FHIR) and the domain-generic Wikidata knowledge base, which is the linked database built and maintained by the Wikimedia Foundation as a sister project to Wikipedia. Example projects that are using Wikidata as a data curation platform are presented as well, along with ways in which they are using ShEx for modeling and validation. When reusing RDF graphs created by others, it is important to know how the data is represented. Current practices of using human-readable descriptions or ontologies to communicate data structures often lack sufficient precision for data consumers to quickly and easily understand data representation details. We provide concrete examples of how we use ShEx as a constraint and validation language that allows humans and machines to communicate unambiguously about data assets. We use ShEx to exchange and understand data models of different origins, and to express a shared model of a resource‚Äôs footprint in a Linked Data source. We also use ShEx to agilely develop data models, test them against sample data, and revise or refine them. The expressivity of ShEx allows us to catch disagreement, inconsistencies, or errors efficiently, both at the time of input, and through batch inspections."
http://videolectures.net/eswc2019_buzio_tinderbook_love/,"More than 2 millions of new books are published every year and choosing a good book among the huge amount of available options can be a challenging endeavor. Recommender systems help in choosing books by providing personalized suggestions based on the user reading history. However, most book recommender systems are based on collaborative filtering, involving a long onboarding process that requires to rate many books before providing good recommendations. Tinderbook provides book recommendations, given a single book that the user likes, through a card-based playful user interface that does not require an account creation. Tinderbook is strongly rooted in semantic technologies, using the DBpedia knowledge graph to enrich book descriptions and extending a hybrid state-of-the-art knowledge graph embeddings algorithm to derive an item relatedness measure for cold start recommendations. Tinderbook is publicly available (http://www.tinderbook.it) and has already generated interest in the public, involving passionate readers, students, librarians, and researchers. The online evaluation shows that Tinderbook achieves almost 50% of precision of the recommendations."
http://videolectures.net/eswc2019_Cifuentes_Gayo_law/,"This paper describes the system architecture for generating the History of the Law developed for the Chilean National Library of Congress (BCN). The production system uses Semantic Web technologies, Akoma-Ntoso, and tools that automate the marking of plain text to XML, enriching and linking documents. These documents semantically annotated allow to develop specialized political and legislative services, and to extract knowledge for a Legal Knowledge Base for public use. We show the strategies used for the implementation of the automatic markup tools, as well as describe the knowledge graph generated from semantic documents. Finally, we show the contrast between the time of document processing using semantic technologies versus manual tasks, and the lessons learnt in this process, installing a base for the replication of a technological model that allows the generation of useful services for diverse contexts."
http://videolectures.net/eswc2019_ristoski_explore_exploit/,"Many Knowledge Extraction systems rely on semantic resources - dictionaries, ontologies, lexical resources - to extract information from unstructured text. A key for successful information extraction is to consider such resources as evolving artifacts and keep them up-to-date. In this paper, we tackle the problem of dictionary expansion and we propose a human-in-the-loop approach: we couple neural language models with tight human supervision to assist the user in building and maintaining domain-specific dictionaries. The approach works on any given input text corpus and is based on the explore and exploit paradigm: starting from a few seeds (or an existing dictionary) it effectively discovers new instances (explore) from the text corpus as well as predicts new potential instances which are not in the corpus, i.e. ‚Äúunseen‚Äù, using the current dictionary entries (exploit). We evaluate our approach on five real-world dictionaries, achieving high accuracy with a rapid expansion rate."
http://videolectures.net/eswc2019_ercan_textual_evidence/,"Knowledge graphs have become vital resources for semantic search and provide users with precise answers to their information needs. Knowledge graphs often consist of billions of facts, typically encoded in the form of RDF triples. In most cases, these facts are extracted automatically and can thus be susceptible to errors. For many applications, it can therefore be very useful to complement knowledge graph facts with textual evidence. For instance, it can help users make informed decisions about the validity of the facts that are returned as part of an answer to a query. In this paper, we therefore propose  Open image in new window, an approach that given a knowledge graph and a text corpus, retrieves the top-k most relevant textual passages for a given set of facts. Since our goal is to retrieve short passages, we develop a set of IR models combining exact matching through the Okapi BM25 model with semantic matching using word embeddings. To evaluate our approach, we built an extensive benchmark consisting of facts extracted from YAGO and text passages retrieved from Wikipedia. Our experimental results demonstrate the effectiveness of our approach in retrieving textual evidence for knowledge graph facts."
http://videolectures.net/eswc2019_car_semantic_web/,"The Location Index (LocI) project is building a national and authoritative, also federated, index for Australian spatial data using Semantic Web technologies. It will be used to link observation and measurement data (social, economic and environmental) to spatial objects identified in any one of multiple, interoperable, datasets. Its goal is to improve efficiency and reliability of data integration to support government decision making."
http://videolectures.net/eswc2019_hyvonen_biographysampo_biographies/,"This paper argues for making a paradigm shift in publishing and using biographical dictionaries on the web, based on Linked Data. The idea is to provide the user with enhanced reading experience of biographies by enriching contents with data linking and reasoning. In addition, versatile tooling for (1) biographical research of individual persons as well as for (2) prosopographical research on groups of people are provided. To demonstrate and evaluate the new possibilities, we present the semantic portal ‚ÄúBiographySampo ‚Äì Finnish Biographies on the Semantic Web‚Äù. The system is based on a knowledge graph extracted automatically from a collection of 13 100 textual biographies, enriched with data linking to 16 external data sources, and by harvesting external collection data from libraries, museums, and archives. The portal was released in September 2018 for free public use at http://biografiasampo.fi."
http://videolectures.net/eswc2019_zhou_toco_ontology/,"The TOUCAN project proposed an ontology for telecommunication networks with hybrid technologies ‚Äì the TOUCAN Ontology (ToCo), available at http://purl.org/toco/, as well as a knowledge design pattern Device-Interface-Link (DIL) pattern. The core classes and relationships forming the ontology are discussed in detail. The ToCo ontology can describe the physical infrastructure, quality of channel, services and users in heterogeneous telecommunication networks which span multiple technology domains. The DIL pattern is observed and summarised when modelling networks with various technology domains. Examples and use cases of ToCo are presented for demonstration."
http://videolectures.net/eswc2019_rossiello_latent_relational/,"Analogy is a fundamental component of the way we think and process thought. Solving a word analogy problem, such as mason is to stone as carpenter is to wood, requires capabilities in recognizing the implicit relations between the two word pairs. In this paper, we describe the analogy problem from a computational linguistics point of view and explore its use to address relation extraction tasks. We extend a relational model that has been shown to be effective in solving word analogies and adapt it to the relation extraction problem. Our experiments show that this approach outperforms the state-of-the-art methods on a relation extraction dataset, opening up a new research direction in discovering implicit relations in text through analogical reasoning."
http://videolectures.net/eswc2019_huang_learning_uri/,"As the Web of Linked Open Data is growing the problem of crawling that cloud becomes increasingly important. Unlike normal Web crawlers, a Linked Data crawler performs a selection to focus on collecting linked RDF (including RDFa) data on the Web. From the perspectives of throughput and coverage, given a newly discovered and targeted URI, the key issue of Linked Data crawlers is to decide whether this URI is likely to dereference into an RDF data source and therefore it is worth downloading the representation it points to. Current solutions adopt heuristic rules to filter irrelevant URIs. Unfortunately, when the heuristics are too restrictive this hampers the coverage of crawling. In this paper, we propose and compare approaches to learn strategies for crawling Linked Data on the Web by predicting whether a newly discovered URI will lead to an RDF data source or not. We detail the features used in predicting the relevance and the methods we evaluated including a promising adaptation of FTRL-proximal online learning algorithm. We compare several options through extensive experiments including existing crawlers as baseline methods to evaluate their efficacy."
http://videolectures.net/eswc2019_scioscia_owl_reasoner/,"Mobile reasoners play a pivotal role in the so-called Semantic Web of Things. While several tools exist for the Android platform, iOS has been neglected so far. This is due to architectural differences and unavailability of OWL manipulation libraries, which make porting existing engines harder. This paper presents Mini-ME Swift, the first Description Logics reasoner for iOS. It implements standard (Subsumption, Satisfiability, Classification, Consistency) and non-standard (Abduction, Contraction, Covering, Difference) inferences in an OWL 2 fragment. Peculiarities are discussed and performance results are presented, comparing Mini-ME Swift with other state-of-the-art OWL reasoners."
http://videolectures.net/eswc2019_kacfah_legal_ontologies/,"Many standards exist to formalize legal texts and rules. The same is true for legal ontologies. However, there is no proof theory to draw conclusions for these ontologically modeled rules. We address this gap by the proposal of a new modeling of deontic statements, and then we use this modeling to propose reasoning mechanisms to answer deontic questions i.e., questions like ‚ÄúIs it mandatory/permitted/prohibited to...‚Äù. We also show that using this modeling, it is possible to check the consistency of a deontic rule base. This work stands as a first important step towards a proof theory over a deontic rule base."
http://videolectures.net/eswc2019_gao_hybrid_graph/,"Distant supervision has advantages of generating training data automatically for relation extraction by aligning triples in Knowledge Graphs with large-scale corpora. Some recent methods attempt to incorporate extra information to enhance the performance of relation extraction. However, there still exist two major limitations. Firstly, these methods are tailored for a specific type of information which is not enough to cover most of the cases. Secondly, the introduced extra information may contain noise. To address these issues, we propose a novel hybrid graph model, which can incorporate heterogeneous background information in a unified framework, such as entity types and human-constructed triples. These various kinds of knowledge can be integrated efficiently even with several missing cases. In addition, we further employ an attention mechanism to identify the most confident information which can alleviate the side effect of noise. Experimental results demonstrate that our model outperforms the state-of-the-art methods significantly in various evaluation metrics."
